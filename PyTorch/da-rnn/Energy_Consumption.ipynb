{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import Tuple\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from modules import Encoder, Decoder\n",
    "from custom_types import DaRnnNet, TrainData, TrainConfig\n",
    "from utils import numpy_to_tvar\n",
    "from constants import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.setup_log()\n",
    "logger.info(f\"Using computation device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dat, col_names) -> Tuple[TrainData, StandardScaler]:\n",
    "    scale = StandardScaler().fit(dat)\n",
    "    #proc_dat = scale.transform(dat)\n",
    "    proc_dat = dat.to_numpy()\n",
    "\n",
    "    mask = np.ones(proc_dat.shape[1], dtype=bool)\n",
    "    dat_cols = list(dat.columns)\n",
    "    for col_name in col_names:\n",
    "        mask[dat_cols.index(col_name)] = False\n",
    "\n",
    "    feats = proc_dat[:, mask]\n",
    "    targs = proc_dat[:, ~mask]\n",
    "\n",
    "    return TrainData(feats, targs), scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def da_rnn(train_data: TrainData, n_targs: int, encoder_hidden_size=64, decoder_hidden_size=64,\n",
    "           T=10, learning_rate=0.01, batch_size=128):\n",
    "\n",
    "    train_cfg = TrainConfig(T, int(train_data.feats.shape[0] * 0.7), batch_size, nn.MSELoss())\n",
    "    logger.info(f\"Training size: {train_cfg.train_size:d}.\")\n",
    "\n",
    "    enc_kwargs = {\"input_size\": train_data.feats.shape[1], \"hidden_size\": encoder_hidden_size, \"T\": T}\n",
    "    encoder = Encoder(**enc_kwargs).to(device)\n",
    "    with open(os.path.join(\"data\", \"enc_kwargs.json\"), \"w\") as fi:\n",
    "        json.dump(enc_kwargs, fi, indent=4)\n",
    "\n",
    "    dec_kwargs = {\"encoder_hidden_size\": encoder_hidden_size,\n",
    "                  \"decoder_hidden_size\": decoder_hidden_size, \"T\": T, \"out_feats\": n_targs}\n",
    "    decoder = Decoder(**dec_kwargs).to(device)\n",
    "    with open(os.path.join(\"data\", \"dec_kwargs.json\"), \"w\") as fi:\n",
    "        json.dump(dec_kwargs, fi, indent=4)\n",
    "\n",
    "    encoder_optimizer = optim.Adam(\n",
    "        params=[p for p in encoder.parameters() if p.requires_grad],\n",
    "        lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(\n",
    "        params=[p for p in decoder.parameters() if p.requires_grad],\n",
    "        lr=learning_rate)\n",
    "    da_rnn_net = DaRnnNet(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
    "\n",
    "    return train_cfg, da_rnn_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net: DaRnnNet, train_data: TrainData, t_cfg: TrainConfig, n_epochs=10, save_plots=False):\n",
    "    iter_per_epoch = int(np.ceil(t_cfg.train_size * 1. / t_cfg.batch_size))\n",
    "    iter_losses = np.zeros(n_epochs * iter_per_epoch)\n",
    "    epoch_losses = np.zeros(n_epochs)\n",
    "    logger.info(f\"Iterations per epoch: {t_cfg.train_size * 1. / t_cfg.batch_size:3.3f} ~ {iter_per_epoch:d}.\")\n",
    "\n",
    "    n_iter = 0\n",
    "\n",
    "    for e_i in range(n_epochs):\n",
    "        perm_idx = np.random.permutation(t_cfg.train_size - t_cfg.T)\n",
    "\n",
    "        for t_i in range(0, t_cfg.train_size, t_cfg.batch_size):\n",
    "            batch_idx = perm_idx[t_i:(t_i + t_cfg.batch_size)]\n",
    "            feats, y_history, y_target = prep_train_data(batch_idx, t_cfg, train_data)\n",
    "\n",
    "            loss = train_iteration(net, t_cfg.loss_func, feats, y_history, y_target)\n",
    "            iter_losses[e_i * iter_per_epoch + t_i // t_cfg.batch_size] = loss\n",
    "            # if (j / t_cfg.batch_size) % 50 == 0:\n",
    "            #    self.logger.info(\"Epoch %d, Batch %d: loss = %3.3f.\", i, j / t_cfg.batch_size, loss)\n",
    "            n_iter += 1\n",
    "\n",
    "            adjust_learning_rate(net, n_iter)\n",
    "\n",
    "        epoch_losses[e_i] = np.mean(iter_losses[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)])\n",
    "\n",
    "        if e_i % 10 == 0:\n",
    "            y_test_pred = predict(net, train_data,\n",
    "                                  t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\n",
    "                                  on_train=False)\n",
    "            # TODO: make this MSE and make it work for multiple inputs\n",
    "            val_loss = y_test_pred - train_data.targs[t_cfg.train_size:]\n",
    "            logger.info(f\"Epoch {e_i:d}, train loss: {epoch_losses[e_i]:3.3f}, val loss: {np.mean(np.abs(val_loss))}.\")\n",
    "            y_train_pred = predict(net, train_data,\n",
    "                                   t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\n",
    "                                   on_train=True)\n",
    "            plt.figure()\n",
    "            plt.plot(range(1, 1 + len(train_data.targs)), train_data.targs,\n",
    "                     label=\"True\")\n",
    "            plt.plot(range(t_cfg.T, len(y_train_pred) + t_cfg.T), y_train_pred,\n",
    "                     label='Predicted - Train')\n",
    "            plt.plot(range(t_cfg.T + len(y_train_pred), len(train_data.targs) + 1), y_test_pred,\n",
    "                     label='Predicted - Test')\n",
    "            plt.legend(loc='upper left')\n",
    "            utils.save_or_show_plot(f\"pred_{e_i}.png\", save_plots)\n",
    "\n",
    "    return iter_losses, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train_data(batch_idx: np.ndarray, t_cfg: TrainConfig, train_data: TrainData):\n",
    "    feats = np.zeros((len(batch_idx), t_cfg.T - 1, train_data.feats.shape[1]))\n",
    "    y_history = np.zeros((len(batch_idx), t_cfg.T - 1, train_data.targs.shape[1]))\n",
    "    y_target = train_data.targs[batch_idx + t_cfg.T]\n",
    "\n",
    "    for b_i, b_idx in enumerate(batch_idx):\n",
    "        b_slc = slice(b_idx, b_idx + t_cfg.T - 1)\n",
    "        feats[b_i, :, :] = train_data.feats[b_slc, :]\n",
    "        y_history[b_i, :] = train_data.targs[b_slc]\n",
    "\n",
    "    return feats, y_history, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(net: DaRnnNet, n_iter: int):\n",
    "    # TODO: Where did this Learning Rate adjustment schedule come from?\n",
    "    # Should be modified to use Cosine Annealing with warm restarts https://www.jeremyjordan.me/nn-learning-rate/\n",
    "    if n_iter % 10000 == 0 and n_iter > 0:\n",
    "        for enc_params, dec_params in zip(net.enc_opt.param_groups, net.dec_opt.param_groups):\n",
    "            enc_params['lr'] = enc_params['lr'] * 0.9\n",
    "            dec_params['lr'] = dec_params['lr'] * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration(t_net: DaRnnNet, loss_func: typing.Callable, X, y_history, y_target):\n",
    "    t_net.enc_opt.zero_grad()\n",
    "    t_net.dec_opt.zero_grad()\n",
    "\n",
    "    input_weighted, input_encoded = t_net.encoder(numpy_to_tvar(X))\n",
    "    y_pred = t_net.decoder(input_encoded, numpy_to_tvar(y_history))\n",
    "\n",
    "    y_true = numpy_to_tvar(y_target)\n",
    "    loss = loss_func(y_pred, y_true)\n",
    "    loss.backward()\n",
    "\n",
    "    t_net.enc_opt.step()\n",
    "    t_net.dec_opt.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(t_net: DaRnnNet, t_dat: TrainData, train_size: int, batch_size: int, T: int, on_train=False):\n",
    "    out_size = t_dat.targs.shape[1]\n",
    "    if on_train:\n",
    "        y_pred = np.zeros((train_size - T + 1, out_size))\n",
    "    else:\n",
    "        y_pred = np.zeros((t_dat.feats.shape[0] - train_size, out_size))\n",
    "\n",
    "    for y_i in range(0, len(y_pred), batch_size):\n",
    "        y_slc = slice(y_i, y_i + batch_size)\n",
    "        batch_idx = range(len(y_pred))[y_slc]\n",
    "        b_len = len(batch_idx)\n",
    "        X = np.zeros((b_len, T - 1, t_dat.feats.shape[1]))\n",
    "        y_history = np.zeros((b_len, T - 1, t_dat.targs.shape[1]))\n",
    "\n",
    "        for b_i, b_idx in enumerate(batch_idx):\n",
    "            if on_train:\n",
    "                idx = range(b_idx, b_idx + T - 1)\n",
    "            else:\n",
    "                idx = range(b_idx + train_size - T, b_idx + train_size - 1)\n",
    "\n",
    "            X[b_i, :, :] = t_dat.feats[idx, :]\n",
    "            y_history[b_i, :] = t_dat.targs[idx]\n",
    "\n",
    "        y_history = numpy_to_tvar(y_history)\n",
    "        _, input_encoded = t_net.encoder(numpy_to_tvar(X))\n",
    "        y_pred[y_slc] = t_net.decoder(input_encoded, y_history).cpu().data.numpy()\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plots = False\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/Normalized 2018.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_consumption = raw_data.groupby('dayofyear')['Energy'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = raw_data[['dayofyear', 'month', 'dayofweek', 'daytype', 'season']].groupby('dayofyear').mean()\n",
    "monthly_data = pd.concat([monthly_data, daily_consumption], axis = 1).reset_index()\n",
    "monthly_data = monthly_data.astype(float)\n",
    "monthly_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_week = monthly_data.shift(7)\n",
    "cols = []\n",
    "for col in prev_week.columns:\n",
    "    cols.append('prev_week_'+col)\n",
    "prev_week.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = pd.concat([monthly_data, prev_week], axis = 1)\n",
    "monthly_data = monthly_data[7:].reset_index(drop = True) #since there is no prev_week data for the first 7 days, I have discarded those rows\n",
    "monthly_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ('Energy',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, scaler = preprocess_data(monthly_data, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_rnn_kwargs = {\"batch_size\": 128, \"T\": 10}\n",
    "config, model = da_rnn(data, n_targs=len(target_cols), learning_rate=.001, **da_rnn_kwargs)\n",
    "iter_loss, epoch_loss = train(model, data, config, n_epochs=500, save_plots=save_plots)\n",
    "final_y_pred = predict(model, data, config.train_size, config.batch_size, config.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(range(len(iter_loss)), iter_loss)\n",
    "utils.save_or_show_plot(\"iter_loss.png\", save_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(range(len(epoch_loss)), epoch_loss)\n",
    "utils.save_or_show_plot(\"epoch_loss.png\", save_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE = {}'.format(mean_squared_error(data.targs[config.train_size:], final_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(final_y_pred, label='Predicted')\n",
    "plt.plot(data.targs[config.train_size:], label=\"True\")\n",
    "plt.legend(loc='upper left')\n",
    "utils.save_or_show_plot(\"final_predicted.png\", save_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"data\", \"da_rnn_kwargs.json\"), \"w\") as fi:\n",
    "    json.dump(da_rnn_kwargs, fi, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scaler, os.path.join(\"data\", \"scaler.pkl\"))\n",
    "torch.save(model.encoder.state_dict(), os.path.join(\"data\", \"encoder.torch\"))\n",
    "torch.save(model.decoder.state_dict(), os.path.join(\"data\", \"decoder.torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
